{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf11401",
   "metadata": {},
   "source": [
    "## 1. Setup and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3faa1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "\n",
      "No GPU detected. Running on CPU.\n",
      "In Colab: Runtime > Change runtime type > Hardware accelerator > GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GPU ENABLED: {len(gpus)} GPU(s) detected\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  GPU {i}: {gpu.name}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo GPU detected. Running on CPU.\")\n",
    "    print(\"In Colab: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307c45d",
   "metadata": {},
   "source": [
    "## 2. Install Required Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023efe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting automatic package installation...\n",
      "\n",
      "ğŸ”§ Installing required packages for blank environment...\n",
      "============================================================\n",
      "\n",
      "ğŸ“¦ Installing tensorflow>=2.10.0...\n",
      "\n",
      "ğŸ“¦ Installing numpy>=1.21.0...\n",
      "\n",
      "ğŸ“¦ Installing pillow>=9.0.0...\n",
      "\n",
      "ğŸ“¦ Installing tqdm>=4.64.0...\n",
      "\n",
      "ğŸ“¦ Installing matplotlib>=3.5.0...\n",
      "\n",
      "============================================================\n",
      "âœ… Package installation completed!\n",
      "============================================================\n",
      "\n",
      "âš ï¸ IMPORTANT: For GPU support, ensure you have:\n",
      "  1. NVIDIA GPU with CUDA Compute Capability 3.5+\n",
      "  2. CUDA Toolkit 11.2 or higher\n",
      "  3. cuDNN 8.1 or higher\n",
      "\n",
      "ğŸ’¡ After installation, restart the kernel if needed.\n",
      "   Kernel > Restart Kernel\n"
     ]
    }
   ],
   "source": [
    "# AUTOMATIC INSTALLATION - Run this cell to install all required packages\n",
    "# This cell will install packages automatically for a blank Python environment\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        'tensorflow>=2.10.0',  # TensorFlow with GPU support\n",
    "        'numpy>=1.21.0',\n",
    "        'pillow>=9.0.0',\n",
    "        'tqdm>=4.64.0',\n",
    "        'matplotlib>=3.5.0',\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ”§ Installing required packages for blank environment...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for package in packages:\n",
    "        print(f\"\\nğŸ“¦ Installing {package}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âš ï¸ Error installing {package}: {e}\")\n",
    "            print(\"Continuing with next package...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Package installation completed!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nâš ï¸ IMPORTANT: For GPU support, ensure you have:\")\n",
    "    print(\"  1. NVIDIA GPU with CUDA Compute Capability 3.5+\")\n",
    "    print(\"  2. CUDA Toolkit 11.2 or higher\")\n",
    "    print(\"  3. cuDNN 8.1 or higher\")\n",
    "    print(\"\\nğŸ’¡ After installation, restart the kernel if needed.\")\n",
    "    print(\"   Kernel > Restart Kernel\")\n",
    "\n",
    "# Run installation automatically\n",
    "print(\"Starting automatic package installation...\\n\")\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395ca172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ requirements.txt created!\n",
      "\n",
      "To install all packages, run in terminal:\n",
      "  pip install -r requirements.txt\n",
      "\n",
      "For GPU support, ensure CUDA and cuDNN are installed.\n"
     ]
    }
   ],
   "source": [
    "# Create requirements.txt file for easy package management\n",
    "requirements_content = \"\"\"# Image Captioning Project Requirements\n",
    "# Install all with: pip install -r requirements.txt\n",
    "\n",
    "tensorflow>=2.10.0\n",
    "numpy>=1.21.0\n",
    "pillow>=9.0.0\n",
    "tqdm>=4.64.0\n",
    "matplotlib>=3.5.0\n",
    "\n",
    "# Optional: For Kaggle dataset download\n",
    "# kaggle>=1.5.12\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"âœ“ requirements.txt created!\")\n",
    "print(\"\\nTo install all packages, run in terminal:\")\n",
    "print(\"  pip install -r requirements.txt\")\n",
    "print(\"\\nFor GPU support, ensure CUDA and cuDNN are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9924cc",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada95fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pickle import dump, load\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import add, concatenate, multiply, RepeatVector, Reshape, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5eb3b3",
   "metadata": {},
   "source": [
    "## 4. Configure Local Dataset Paths\n",
    "\n",
    "Set up paths to your local Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce545fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Configuration:\n",
      "  Text files: C:\\Users\\jebar\\OneDrive\\Bureau\\Deep Learning\\Flickr8k\\Flickr8k_text\n",
      "  Images: C:\\Users\\jebar\\OneDrive\\Bureau\\Deep Learning\\Flickr8k\\Flicker8k_Dataset\n",
      "\n",
      "âœ“ Dataset paths found!\n",
      "  Images found: 8091 files\n"
     ]
    }
   ],
   "source": [
    "# LOCAL SETUP: Configure your dataset paths here\n",
    "# Download Flickr8k from: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
    "\n",
    "# Option 1: Set your local paths (RECOMMENDED)\n",
    "base_path = r\"path to your dataset\"\n",
    "dataset_text = os.path.join(base_path, \"Flickr8k_text\")\n",
    "dataset_images = os.path.join(base_path, \"Flicker8k_Dataset\")\n",
    "\n",
    "# Option 2: Or use relative paths if dataset is in the same directory\n",
    "# base_path = \"Flickr8k\"\n",
    "# dataset_text = os.path.join(base_path, \"Flickr8k_text\")\n",
    "# dataset_images = os.path.join(base_path, \"Flicker8k_Dataset\")\n",
    "\n",
    "print(\"Dataset Configuration:\")\n",
    "print(f\"  Text files: {dataset_text}\")\n",
    "print(f\"  Images: {dataset_images}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if os.path.exists(dataset_text) and os.path.exists(dataset_images):\n",
    "    print(\"\\nâœ“ Dataset paths found!\")\n",
    "    print(f\"  Images found: {len([f for f in os.listdir(dataset_images) if f.endswith(('.jpg', '.png'))])} files\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: Dataset paths not found!\")\n",
    "    print(\"  Please update the paths above to point to your Flickr8k dataset location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98971a72",
   "metadata": {},
   "source": [
    "## 5. Download Dataset (If Needed)\n",
    "\n",
    "If you don't have the Flickr8k dataset yet, download it from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f0a07",
   "metadata": {},
   "source": [
    "## 6. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c02e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    \"\"\"Load a text file into memory\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def all_img_captions(filename):\n",
    "    \"\"\"Extract all image captions from the token file\"\"\"\n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n",
    "\n",
    "print(\"Data loading functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fb5c1",
   "metadata": {},
   "source": [
    "## 7. Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50ba1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning functions defined!\n"
     ]
    }
   ],
   "source": [
    "def cleaning_text(captions):\n",
    "    \"\"\"Clean and preprocess text captions\"\"\"\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caps in captions.items():\n",
    "        for i, img_caption in enumerate(caps):\n",
    "            img_caption = img_caption.replace(\"-\", \" \")\n",
    "            desc = img_caption.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            desc = [word for word in desc if len(word) > 1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i] = img_caption\n",
    "    return captions\n",
    "\n",
    "def text_vocabulary(descriptions):\n",
    "    \"\"\"Build vocabulary of all unique words\"\"\"\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab\n",
    "\n",
    "def save_descriptions(descriptions, filename):\n",
    "    \"\"\"Save descriptions to file\"\"\"\n",
    "    lines = []\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc)\n",
    "    data = \"\\n\".join(lines)\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(data)\n",
    "\n",
    "print(\"Text cleaning functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dec212",
   "metadata": {},
   "source": [
    "## 8. Prepare Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99aae2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing captions...\n",
      "Loaded 8092 images with captions\n",
      "Vocabulary size: 8422 words\n",
      "descriptions.txt created successfully!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"descriptions.txt\"):\n",
    "    print(\"descriptions.txt already exists. Skipping preprocessing...\")\n",
    "else:\n",
    "    print(\"Processing captions...\")\n",
    "    filename = os.path.join(dataset_text, \"Flickr8k.token.txt\")\n",
    "    \n",
    "    # Load all captions\n",
    "    descriptions = all_img_captions(filename)\n",
    "    print(f\"Loaded {len(descriptions)} images with captions\")\n",
    "    \n",
    "    # Clean descriptions\n",
    "    clean_descriptions = cleaning_text(descriptions)\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocabulary = text_vocabulary(clean_descriptions)\n",
    "    print(f\"Vocabulary size: {len(vocabulary)} words\")\n",
    "    \n",
    "    # Save to file\n",
    "    save_descriptions(clean_descriptions, \"descriptions.txt\")\n",
    "    print(\"descriptions.txt created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c6490",
   "metadata": {},
   "source": [
    "## 9. Feature Extraction with Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca1e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, batch_size=16):\n",
    "    \"\"\"Extract features from images using Xception model\"\"\"\n",
    "    # Load Xception model\n",
    "    model = Xception(include_top=False, pooling='avg', weights='imagenet')\n",
    "    print(\"Xception model loaded!\")\n",
    "    \n",
    "    features = {}\n",
    "    valid_images = ['.jpg', '.jpeg', '.png']\n",
    "    \n",
    "    # Get all image paths\n",
    "    image_paths = []\n",
    "    for img in os.listdir(directory):\n",
    "        ext = os.path.splitext(img)[1].lower()\n",
    "        if ext in valid_images:\n",
    "            image_paths.append(img)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images to process\")\n",
    "    \n",
    "    # Process in batches for efficiency on GPU\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Extracting features\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        valid_batch_names = []\n",
    "        \n",
    "        for img in batch_paths:\n",
    "            filepath = os.path.join(directory, img)\n",
    "            try:\n",
    "                image = Image.open(filepath)\n",
    "                image = image.resize((299, 299))\n",
    "                image = np.array(image)\n",
    "                \n",
    "                # Handle grayscale\n",
    "                if len(image.shape) == 2:\n",
    "                    image = np.stack([image] * 3, axis=-1)\n",
    "                # Handle RGBA\n",
    "                elif image.shape[2] == 4:\n",
    "                    image = image[:, :, :3]\n",
    "                \n",
    "                image = preprocess_input(image)\n",
    "                batch_images.append(image)\n",
    "                valid_batch_names.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_images:\n",
    "            batch_array = np.array(batch_images)\n",
    "            batch_features = model.predict(batch_array, verbose=0)\n",
    "            \n",
    "            for idx, img_name in enumerate(valid_batch_names):\n",
    "                features[img_name] = batch_features[idx].reshape(1, -1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da103d00",
   "metadata": {},
   "source": [
    "## 10. Extract and Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158bbddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from images...\n",
      "This may take several minutes with GPU...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Xception model loaded!\n",
      "Found 8091 images to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 506/506 [26:00<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted and saved for 8091 images!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"features.pkl\"):\n",
    "    print(\"features.pkl already exists. Loading features...\")\n",
    "    features = load(open(\"features.pkl\", \"rb\"))\n",
    "    print(f\"Loaded features for {len(features)} images\")\n",
    "else:\n",
    "    print(\"Extracting features from images...\")\n",
    "    print(\"This may take several minutes with GPU...\")\n",
    "    \n",
    "    features = extract_features(dataset_images, batch_size=16)\n",
    "    \n",
    "    # Save features\n",
    "    dump(features, open(\"features.pkl\", \"wb\"))\n",
    "    print(f\"Features extracted and saved for {len(features)} images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333fb901",
   "metadata": {},
   "source": [
    "## 11. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19bdbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 6000 images\n",
      "Loaded descriptions for 6000 images\n",
      "Loaded features for 6000 images\n"
     ]
    }
   ],
   "source": [
    "def load_photos(filename):\n",
    "    \"\"\"Load list of photo identifiers\"\"\"\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    return photos\n",
    "\n",
    "def load_clean_descriptions(filename, photos):\n",
    "    \"\"\"Load clean descriptions for photos\"\"\"\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "def load_features(photos, all_features):\n",
    "    \"\"\"Load features for specific photos\"\"\"\n",
    "    return {k: all_features[k] for k in photos if k in all_features}\n",
    "\n",
    "# Load training set\n",
    "train_filename = os.path.join(dataset_text, \"Flickr_8k.trainImages.txt\")\n",
    "train_imgs = load_photos(train_filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs, features)\n",
    "\n",
    "print(f\"Training set: {len(train_imgs)} images\")\n",
    "print(f\"Loaded descriptions for {len(train_descriptions)} images\")\n",
    "print(f\"Loaded features for {len(train_features)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ac01e",
   "metadata": {},
   "source": [
    "## 12. Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03468b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenizer...\n",
      "Tokenizer created and saved!\n",
      "Vocabulary size: 7318\n"
     ]
    }
   ],
   "source": [
    "def dict_to_list(descriptions):\n",
    "    \"\"\"Convert dictionary to list of descriptions\"\"\"\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"Create tokenizer for text\"\"\"\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer\n",
    "\n",
    "# Create and save tokenizer\n",
    "if os.path.exists('tokenizer.pkl'):\n",
    "    print(\"Loading existing tokenizer...\")\n",
    "    tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "else:\n",
    "    print(\"Creating tokenizer...\")\n",
    "    tokenizer = create_tokenizer(train_descriptions)\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    print(\"Tokenizer created and saved!\")\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33433353",
   "metadata": {},
   "source": [
    "## 13. Calculate Maximum Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6811a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 35\n"
     ]
    }
   ],
   "source": [
    "def max_length(descriptions):\n",
    "    \"\"\"Calculate maximum length of descriptions\"\"\"\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print(f\"Maximum sequence length: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73e70b",
   "metadata": {},
   "source": [
    "## 14. Data Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f255165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced data generator with label smoothing defined!\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list, feature, vocab_size, label_smoothing=0.1):\n",
    "    \"\"\"Create input-output sequence pairs with optional label smoothing\"\"\"\n",
    "    X1, X2, y = [], [], []\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # CRITICAL FIX: Use padding='post' for right-padding (required for cuDNN LSTM on GPU)\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            \n",
    "            # Apply label smoothing for better generalization\n",
    "            if label_smoothing > 0:\n",
    "                out_seq = out_seq * (1 - label_smoothing) + (label_smoothing / vocab_size)\n",
    "            \n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "def data_generator(descriptions, features, tokenizer, max_length, vocab_size, batch_size=32, label_smoothing=0.1):\n",
    "    \"\"\"Generate batches of data for training with label smoothing\"\"\"\n",
    "    def generator():\n",
    "        while True:\n",
    "            for key, description_list in descriptions.items():\n",
    "                if key not in features:\n",
    "                    continue\n",
    "                feature = features[key][0]\n",
    "                input_image, input_sequence, output_word = create_sequences(\n",
    "                    tokenizer, max_length, description_list, feature, vocab_size, label_smoothing\n",
    "                )\n",
    "                for i in range(len(input_image)):\n",
    "                    yield {'input_1': input_image[i], 'input_2': input_sequence[i]}, output_word[i]\n",
    "    \n",
    "    output_signature = (\n",
    "        {\n",
    "            'input_1': tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
    "            'input_2': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Enhanced data generator with label smoothing defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434da2f0",
   "metadata": {},
   "source": [
    "## 15. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6996c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model architecture with gating mechanism defined!\n"
     ]
    }
   ],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    \"\"\"Define the enhanced image captioning model with attention-like mechanism\"\"\"\n",
    "    # Image feature input - Deeper processing with residual connection\n",
    "    inputs1 = Input(shape=(2048,), name='input_1')\n",
    "    fe1 = Dense(512, activation='relu')(inputs1)\n",
    "    fe2 = BatchNormalization()(fe1)\n",
    "    fe3 = Dropout(0.4)(fe2)\n",
    "    fe4 = Dense(512, activation='relu')(fe3)\n",
    "    fe5 = BatchNormalization()(fe4)\n",
    "    # Residual connection\n",
    "    fe_residual = add([fe1, fe4])\n",
    "    fe6 = Dense(256, activation='relu')(fe_residual)\n",
    "    \n",
    "    # Sequence input - Deeper LSTM with more capacity\n",
    "    inputs2 = Input(shape=(max_length,), name='input_2')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.4)(se1)\n",
    "    # First LSTM layer with return_sequences\n",
    "    se3 = LSTM(512, return_sequences=True, recurrent_dropout=0.2)(se2)\n",
    "    se4 = BatchNormalization()(se3)\n",
    "    # Second LSTM layer\n",
    "    se5 = LSTM(512, return_sequences=False, recurrent_dropout=0.2)(se4)\n",
    "    se6 = BatchNormalization()(se5)\n",
    "    se7 = Dense(256, activation='relu')(se6)\n",
    "    \n",
    "    # Advanced decoder with gating mechanism (attention-like)\n",
    "    # Create a gating vector to weight image and text features\n",
    "    combined = concatenate([fe6, se7])\n",
    "    \n",
    "    # Gating mechanism - learns to balance image vs text importance\n",
    "    gate = Dense(256, activation='sigmoid', name='gate')(combined)\n",
    "    gated_image = multiply([fe6, gate])\n",
    "    gated_text = multiply([se7, gate])\n",
    "    \n",
    "    # Combine gated features\n",
    "    decoder1 = concatenate([gated_image, gated_text])\n",
    "    decoder2 = Dense(512, activation='relu')(decoder1)\n",
    "    decoder3 = BatchNormalization()(decoder2)\n",
    "    decoder4 = Dropout(0.5)(decoder3)\n",
    "    decoder5 = Dense(512, activation='relu')(decoder4)\n",
    "    decoder6 = BatchNormalization()(decoder5)\n",
    "    decoder7 = Dropout(0.5)(decoder6)\n",
    "    decoder8 = Dense(256, activation='relu')(decoder7)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder8)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    \n",
    "    # Use a more sophisticated optimizer configuration\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0003,  # Slightly higher initial LR\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Enhanced model architecture with gating mechanism defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13e4eb",
   "metadata": {},
   "source": [
    "## 16. Create Model and View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2914fb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ input_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,873,408</span> â”‚ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> â”‚ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_4         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalization_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_6         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> â”‚ batch_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_7         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ batch_normalization_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], gate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], gate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],            â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_8         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalization_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_9         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalization_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7318</span>)              â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,880,726</span> â”‚ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_2 (\u001b[38;5;33mInputLayer\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ input_1 (\u001b[38;5;33mInputLayer\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m256\u001b[0m)           â”‚       \u001b[38;5;34m1,873,408\u001b[0m â”‚ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚       \u001b[38;5;34m1,049,088\u001b[0m â”‚ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m256\u001b[0m)           â”‚               \u001b[38;5;34m0\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_4         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚           \u001b[38;5;34m2,048\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m512\u001b[0m)           â”‚       \u001b[38;5;34m1,574,912\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ batch_normalization_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_6         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m512\u001b[0m)           â”‚           \u001b[38;5;34m2,048\u001b[0m â”‚ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],                â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚         \u001b[38;5;34m262,656\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚       \u001b[38;5;34m2,099,200\u001b[0m â”‚ batch_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_12 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_7         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚           \u001b[38;5;34m2,048\u001b[0m â”‚ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚ batch_normalization_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gate (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply (\u001b[38;5;33mMultiply\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], gate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], gate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],            â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚         \u001b[38;5;34m262,656\u001b[0m â”‚ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_8         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚           \u001b[38;5;34m2,048\u001b[0m â”‚ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ batch_normalization_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚         \u001b[38;5;34m262,656\u001b[0m â”‚ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_9         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚           \u001b[38;5;34m2,048\u001b[0m â”‚ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ batch_normalization_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7318\u001b[0m)              â”‚       \u001b[38;5;34m1,880,726\u001b[0m â”‚ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,800,854</span> (37.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,800,854\u001b[0m (37.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,795,734</span> (37.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,795,734\u001b[0m (37.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> (20.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,120\u001b[0m (20.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Steps per epoch: 9596\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)\n",
    "print(model.summary())\n",
    "\n",
    "# Calculate training steps\n",
    "def get_steps_per_epoch(descriptions, batch_size=32):\n",
    "    total_sequences = 0\n",
    "    for img_captions in descriptions.values():\n",
    "        for caption in img_captions:\n",
    "            words = caption.split()\n",
    "            total_sequences += len(words) - 1\n",
    "    return max(1, total_sequences // batch_size)\n",
    "\n",
    "batch_size = 32\n",
    "steps_per_epoch = get_steps_per_epoch(train_descriptions, batch_size)\n",
    "print(f\"\\nSteps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed9e66",
   "metadata": {},
   "source": [
    "## 17. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c63965ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced callbacks configured with learning rate warmup!\n"
     ]
    }
   ],
   "source": [
    "# Create models directory\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='models/best_model.keras',\n",
    "    monitor='loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# More aggressive learning rate schedule\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.3,  # Reduce by 70%\n",
    "    patience=2,\n",
    "    min_lr=0.00000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Add early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=6,  # More patience for better convergence\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Add learning rate warmup using custom callback\n",
    "class WarmUpLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, warmup_epochs=3, initial_lr=0.00003, target_lr=0.0003):\n",
    "        super(WarmUpLearningRate, self).__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        self.target_lr = target_lr\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Gradually increase learning rate\n",
    "            lr = self.initial_lr + (self.target_lr - self.initial_lr) * (epoch / self.warmup_epochs)\n",
    "            # Fix for newer TensorFlow versions - use learning_rate instead of lr\n",
    "            self.model.optimizer.learning_rate.assign(lr)\n",
    "            print(f\"Warmup: Setting learning rate to {lr:.6f}\")\n",
    "\n",
    "warmup = WarmUpLearningRate(warmup_epochs=3, initial_lr=0.00003, target_lr=0.0003)\n",
    "\n",
    "print(\"Enhanced callbacks configured with learning rate warmup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce848c3",
   "metadata": {},
   "source": [
    "## 18. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a0fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Training Configuration:\n",
      "  Epochs: 25\n",
      "  Batch size: 64 (increased for stability)\n",
      "  Steps per epoch: 4798\n",
      "  Vocabulary size: 7318\n",
      "  Max sequence length: 35\n",
      "\n",
      "Optimizations:\n",
      "  âœ“ Gating mechanism for feature fusion\n",
      "  âœ“ Residual connections in encoder\n",
      "  âœ“ Deeper LSTM with recurrent dropout\n",
      "  âœ“ Learning rate warmup (3 epochs)\n",
      "  âœ“ Aggressive LR reduction on plateau\n",
      "\n",
      "Starting training...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 1/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1093 - loss: 6.7520\n",
      "Epoch 1: loss improved from None to 6.20228, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6330s\u001b[0m 1s/step - accuracy: 0.1347 - loss: 6.2023 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_1.keras\n",
      "\n",
      "Epoch 1 Results:\n",
      "  Loss: 6.2023\n",
      "  Accuracy: 0.1347 (13.47%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: inf\n",
      "\n",
      "============================================================\n",
      "Epoch 2/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1770 - loss: 5.6722\n",
      "Epoch 1: loss improved from 6.20228 to 5.59886, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13191s\u001b[0m 3s/step - accuracy: 0.1902 - loss: 5.5989 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_2.keras\n",
      "\n",
      "Epoch 2 Results:\n",
      "  Loss: 5.5989\n",
      "  Accuracy: 0.1902 (19.02%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.6034\n",
      "\n",
      "============================================================\n",
      "Epoch 3/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2296 - loss: 5.2685\n",
      "Epoch 1: loss improved from 5.59886 to 5.22789, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13306s\u001b[0m 3s/step - accuracy: 0.2367 - loss: 5.2279 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_3.keras\n",
      "\n",
      "Epoch 3 Results:\n",
      "  Loss: 5.2279\n",
      "  Accuracy: 0.2367 (23.67%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.3710\n",
      "\n",
      "============================================================\n",
      "Epoch 4/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2589 - loss: 5.0217\n",
      "Epoch 1: loss improved from 5.22789 to 5.01202, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13344s\u001b[0m 3s/step - accuracy: 0.2612 - loss: 5.0120 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_4.keras\n",
      "\n",
      "Epoch 4 Results:\n",
      "  Loss: 5.0120\n",
      "  Accuracy: 0.2612 (26.12%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.2159\n",
      "\n",
      "============================================================\n",
      "Epoch 5/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2767 - loss: 4.8702\n",
      "Epoch 1: loss improved from 5.01202 to 4.86843, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13456s\u001b[0m 3s/step - accuracy: 0.2777 - loss: 4.8684 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_5.keras\n",
      "\n",
      "Epoch 5 Results:\n",
      "  Loss: 4.8684\n",
      "  Accuracy: 0.2777 (27.77%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.1436\n",
      "\n",
      "============================================================\n",
      "Epoch 6/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2925 - loss: 4.7518\n",
      "Epoch 1: loss improved from 4.86843 to 4.75291, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13783s\u001b[0m 3s/step - accuracy: 0.2933 - loss: 4.7529 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_6.keras\n",
      "\n",
      "Epoch 6 Results:\n",
      "  Loss: 4.7529\n",
      "  Accuracy: 0.2933 (29.33%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.1155\n",
      "\n",
      "============================================================\n",
      "Epoch 7/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3061 - loss: 4.6550\n",
      "Epoch 1: loss improved from 4.75291 to 4.65955, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6825s\u001b[0m 1s/step - accuracy: 0.3070 - loss: 4.6595 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_7.keras\n",
      "\n",
      "Epoch 7 Results:\n",
      "  Loss: 4.6595\n",
      "  Accuracy: 0.3070 (30.70%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0934\n",
      "\n",
      "============================================================\n",
      "Epoch 8/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975ms/step - accuracy: 0.3187 - loss: 4.5695\n",
      "Epoch 1: loss improved from 4.65955 to 4.57486, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4677s\u001b[0m 975ms/step - accuracy: 0.3183 - loss: 4.5749 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_8.keras\n",
      "\n",
      "Epoch 8 Results:\n",
      "  Loss: 4.5749\n",
      "  Accuracy: 0.3183 (31.83%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0847\n",
      "\n",
      "============================================================\n",
      "Epoch 9/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3293 - loss: 4.4980\n",
      "Epoch 1: loss improved from 4.57486 to 4.50455, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6797s\u001b[0m 1s/step - accuracy: 0.3283 - loss: 4.5045 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_9.keras\n",
      "\n",
      "Epoch 9 Results:\n",
      "  Loss: 4.5045\n",
      "  Accuracy: 0.3283 (32.83%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0703\n",
      "\n",
      "============================================================\n",
      "Epoch 10/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929ms/step - accuracy: 0.3386 - loss: 4.4324\n",
      "Epoch 1: loss improved from 4.50455 to 4.43973, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4459s\u001b[0m 929ms/step - accuracy: 0.3377 - loss: 4.4397 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_10.keras\n",
      "\n",
      "Epoch 10 Results:\n",
      "  Loss: 4.4397\n",
      "  Accuracy: 0.3377 (33.77%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0648\n",
      "\n",
      "============================================================\n",
      "Epoch 11/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3471 - loss: 4.3742\n",
      "Epoch 1: loss improved from 4.43973 to 4.38147, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7696s\u001b[0m 2s/step - accuracy: 0.3458 - loss: 4.3815 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_11.keras\n",
      "\n",
      "Epoch 11 Results:\n",
      "  Loss: 4.3815\n",
      "  Accuracy: 0.3458 (34.58%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0583\n",
      "\n",
      "============================================================\n",
      "Epoch 12/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3554 - loss: 4.3199\n",
      "Epoch 1: loss improved from 4.38147 to 4.32617, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6322s\u001b[0m 1s/step - accuracy: 0.3545 - loss: 4.3262 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_12.keras\n",
      "\n",
      "Epoch 12 Results:\n",
      "  Loss: 4.3262\n",
      "  Accuracy: 0.3545 (35.45%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0553\n",
      "\n",
      "============================================================\n",
      "Epoch 13/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3642 - loss: 4.2652\n",
      "Epoch 1: loss improved from 4.32617 to 4.27271, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5790s\u001b[0m 1s/step - accuracy: 0.3626 - loss: 4.2727 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_13.keras\n",
      "\n",
      "Epoch 13 Results:\n",
      "  Loss: 4.2727\n",
      "  Accuracy: 0.3626 (36.26%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0535\n",
      "\n",
      "============================================================\n",
      "Epoch 14/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3705 - loss: 4.2184\n",
      "Epoch 1: loss improved from 4.27271 to 4.22547, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7947s\u001b[0m 2s/step - accuracy: 0.3694 - loss: 4.2255 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_14.keras\n",
      "\n",
      "Epoch 14 Results:\n",
      "  Loss: 4.2255\n",
      "  Accuracy: 0.3694 (36.94%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0472\n",
      "\n",
      "============================================================\n",
      "Epoch 15/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3794 - loss: 4.1704\n",
      "Epoch 1: loss improved from 4.22547 to 4.17845, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7983s\u001b[0m 2s/step - accuracy: 0.3769 - loss: 4.1785 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_15.keras\n",
      "\n",
      "Epoch 15 Results:\n",
      "  Loss: 4.1785\n",
      "  Accuracy: 0.3769 (37.69%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0470\n",
      "\n",
      "============================================================\n",
      "Epoch 16/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3841 - loss: 4.1316\n",
      "Epoch 1: loss improved from 4.17845 to 4.13820, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8268s\u001b[0m 2s/step - accuracy: 0.3828 - loss: 4.1382 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_16.keras\n",
      "\n",
      "Epoch 16 Results:\n",
      "  Loss: 4.1382\n",
      "  Accuracy: 0.3828 (38.28%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0403\n",
      "\n",
      "============================================================\n",
      "Epoch 17/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3904 - loss: 4.0888\n",
      "Epoch 1: loss improved from 4.13820 to 4.09728, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8336s\u001b[0m 2s/step - accuracy: 0.3883 - loss: 4.0973 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_17.keras\n",
      "\n",
      "Epoch 17 Results:\n",
      "  Loss: 4.0973\n",
      "  Accuracy: 0.3883 (38.83%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0409\n",
      "\n",
      "============================================================\n",
      "Epoch 18/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3961 - loss: 4.0500\n",
      "Epoch 1: loss improved from 4.09728 to 4.05662, saving model to models/best_model.keras\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5233s\u001b[0m 1s/step - accuracy: 0.3946 - loss: 4.0566 - learning_rate: 3.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Model saved: models/model_epoch_18.keras\n",
      "\n",
      "Epoch 18 Results:\n",
      "  Loss: 4.0566\n",
      "  Accuracy: 0.3946 (39.46%)\n",
      "  Learning Rate: 0.00003000\n",
      "  âœ“ Improvement: 0.0407\n",
      "\n",
      "============================================================\n",
      "Epoch 19/25\n",
      "============================================================\n",
      "Warmup: Setting learning rate to 0.000030\n",
      "\u001b[1m   7/4798\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:23:58\u001b[0m 1s/step - accuracy: 0.4232 - loss: 3.8587"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m\n\u001b[0;32m     32\u001b[0m dataset \u001b[38;5;241m=\u001b[39m data_generator(\n\u001b[0;32m     33\u001b[0m     train_descriptions,\n\u001b[0;32m     34\u001b[0m     train_features,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     batch_size\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train for one epoch with all callbacks\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save model after each epoch\u001b[39;00m\n\u001b[0;32m     51\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/model_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jebar\\miniconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "epochs = 25  # More epochs with early stopping\n",
    "batch_size = 64  # Larger batch size for more stable gradients\n",
    "\n",
    "# Recalculate steps with new batch size\n",
    "steps_per_epoch = get_steps_per_epoch(train_descriptions, batch_size)\n",
    "\n",
    "print(f\"\\nEnhanced Training Configuration:\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Batch size: {batch_size} (increased for stability)\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Max sequence length: {max_length}\")\n",
    "print(f\"\\nOptimizations:\")\n",
    "print(f\"  âœ“ Gating mechanism for feature fusion\")\n",
    "print(f\"  âœ“ Residual connections in encoder\")\n",
    "print(f\"  âœ“ Deeper LSTM with recurrent dropout\")\n",
    "print(f\"  âœ“ Learning rate warmup (3 epochs)\")\n",
    "print(f\"  âœ“ Aggressive LR reduction on plateau\")\n",
    "print(f\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train the model\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create fresh dataset for each epoch\n",
    "    dataset = data_generator(\n",
    "        train_descriptions,\n",
    "        train_features,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        batch_size\n",
    "    )\n",
    "    \n",
    "    # Train for one epoch with all callbacks\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[checkpoint, reduce_lr, early_stop, warmup],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model after each epoch\n",
    "    model_path = f\"models/model_epoch_{epoch + 1}.keras\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    # Save as latest\n",
    "    model.save(\"models/latest_model.keras\")\n",
    "    \n",
    "    # Print metrics\n",
    "    loss = history.history['loss'][0]\n",
    "    acc = history.history['accuracy'][0]\n",
    "    # Fix for newer TensorFlow versions - use learning_rate instead of lr\n",
    "    current_lr = float(model.optimizer.learning_rate.numpy())\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Loss: {loss:.4f}\")\n",
    "    print(f\"  Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    print(f\"  Learning Rate: {current_lr:.8f}\")\n",
    "    \n",
    "    # Track improvement\n",
    "    if loss < best_loss:\n",
    "        improvement = best_loss - loss\n",
    "        best_loss = loss\n",
    "        patience_counter = 0\n",
    "        print(f\"  âœ“ Improvement: {improvement:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  âš  No improvement (patience: {patience_counter}/6)\")\n",
    "    \n",
    "    # Check if early stopping triggered\n",
    "    if early_stop.stopped_epoch > 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        print(f\"Best loss achieved: {best_loss:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final best loss: {best_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
